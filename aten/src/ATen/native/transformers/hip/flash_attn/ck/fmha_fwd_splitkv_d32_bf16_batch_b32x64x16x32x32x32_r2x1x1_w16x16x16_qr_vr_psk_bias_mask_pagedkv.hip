// ==========================================
// THIS CODE IS AUTOGENERATED. DO NOT MODIFY.
// @generated
// ==========================================
// SPDX-License-Identifier: MIT
// Copyright (c) 2018-2024, Advanced Micro Devices, Inc. All rights reserved.

// auto generated by generate.py
#include "fmha_fwd.hpp"

using fmha_dtype_0 = ck_tile::bf16_t;
using fmha_mask_0 = ck_tile::SimplifiedGenericAttentionMask<true>;

namespace {
template <bool kHasUnevenSplits>
struct kernel_runner {
using fmha_block_tile = ck_tile::sequence<32, 64, 16, 32, 32, 32>;
using fmha_block_warps = ck_tile::sequence<2, 1, 1>;
using fmha_warp_tile = ck_tile::sequence<16, 16, 16>;

using fmha_shape = ck_tile::TileFmhaShape<fmha_block_tile,
                                          fmha_block_warps,
                                          fmha_warp_tile,
                                          fmha_block_warps,
                                          fmha_warp_tile,
                                          true>;

using fmha_trait = ck_tile::TileFmhaFwdSplitKVTraits<false,
                                                     true,
                                                     false,
                                                     false,
                                                     ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS,
                                                     false,
                                                     false,
                                                     false,
                                                     true,
                                                     kHasUnevenSplits,
                                                     -1>;

using fmha_pipeline_problem = ck_tile::BlockFmhaFwdSplitKVPipelineProblem<
    typename FmhaFwdTypeConfig<fmha_dtype_0>::QDataType,
    typename FmhaFwdTypeConfig<fmha_dtype_0>::KDataType,
    typename FmhaFwdTypeConfig<fmha_dtype_0>::VDataType,
    typename FmhaFwdTypeConfig<fmha_dtype_0>::SaccDataType,
    typename FmhaFwdTypeConfig<fmha_dtype_0>::SMPLComputeDataType,
    typename FmhaFwdTypeConfig<fmha_dtype_0>::BiasDataType,
    typename FmhaFwdTypeConfig<fmha_dtype_0>::LSEDataType,
    typename FmhaFwdTypeConfig<fmha_dtype_0>::PDataType,
    typename FmhaFwdTypeConfig<fmha_dtype_0>::OaccDataType,
    typename FmhaFwdTypeConfig<fmha_dtype_0>::OaccDataType,
    fmha_shape,
    false,
    fmha_mask_0,
    fmha_trait>;

using fmha_pipeline = ck_tile::BlockFmhaFwdSplitKVPipelineQRKSVS<
    fmha_pipeline_problem>;

using fmha_epilogue =
    ck_tile::Default2DEpilogue<ck_tile::Default2DEpilogueProblem<typename FmhaFwdTypeConfig<ck_tile::bf16_t>::OaccDataType,
                                           typename FmhaFwdTypeConfig<ck_tile::bf16_t>::OaccDataType,
                                           false, false>>;

using fmha_kernel =
    ck_tile::FmhaFwdSplitKVKernel<ck_tile::FmhaFwdSplitKVTilePartitioner<fmha_shape>,
                  fmha_pipeline,
                  fmha_epilogue>;

static void run(const ck_tile::stream_config& s, fmha_fwd_splitkv_args a)
{
    using k_ = fmha_kernel;
    auto [kargs, grids] = fmha_fwd_splitkv_create_kargs_and_grids<k_>(a);
    constexpr dim3 blocks             = k_::BlockSize();
    constexpr ck_tile::index_t kBlockPerCu = k_::kBlockPerCu;
    ck_tile::make_kernel<blocks.x, kBlockPerCu>(k_{}, grids, blocks, 0, kargs)(ck_tile::stream_config{s.stream_id_});
}
};
}

using trait_0 = fmha_fwd_splitkv_traits_<32, ck_tile::bf16_t, false, 32, 64, 16, 32, 32, 32, true,
                        ck_tile::BlockFmhaPipelineEnum::QRKSVS, fmha_mask_0, ck_tile::BlockAttentionBiasEnum::ELEMENTWISE_BIAS, false, false, true, false, true, false, 
                        false>;

#include <iostream>

template<>
void fmha_fwd_splitkv_oneshot_<trait_0>(const ck_tile::stream_config& s, fmha_fwd_splitkv_args a)
{
    if constexpr(false == false) { // batch mode
        // we don't check every seqlen_k values for kvcache
        if (a.seqlen_k_ptr != nullptr) {
            kernel_runner<true>::run(s, a);
        // make sure F_bn0 is divisible by F_bk1
        } else if (a.seqlen_k % (a.num_splits * 64) == 0) {
            kernel_runner<false>::run(s, a);
        } else {
            kernel_runner<true>::run(s, a);
        }
    } else {
        kernel_runner<true>::run(s, a);
    }
}

template<>
std::string fmha_fwd_splitkv_get_name_<trait_0>()
{
    using k_ = kernel_runner<true>::fmha_kernel; /// FIXME: choose real kernel type
    return k_::GetName();
}
